{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils import *       #changed\n",
    "from models import TransformerLM, TransformerConditionedLM\n",
    "import pickle\n",
    "from imageio import imread\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "# /net/papilio/storage2/yhaoyuan/transformer_I2S/saved_model/I2U/trimmed_mapping_SpeakerALL_Color/Trial_1/bleu-4_BEST_checkpoint_coco_2_cap_per_img_1_min_word_freq_gpu.pth.tar\n",
    "# checkpoint_path = \"../../saved_model/I2U/trimmed_mapping_SpeakerALL/Trial_3/\"\n",
    "# checkpoint_path = \"../../saved_model/I2U/trimmed_mapping_SpeakerALL_Color/Trial_1/\"\n",
    "checkpoint_path = \"../../saved_model/I2U/VC_5_captions/Trial_1/\"\n",
    "\n",
    "with open(checkpoint_path + 'config.yml', 'r') as yml:\n",
    "    config = yaml.safe_load(yml)\n",
    "\n",
    "checkpoint = checkpoint_path + f'bleu-4_BEST_checkpoint_coco_{str(config[\"i2u\"][\"captions_per_image\"])}_cap_per_img_{str(config[\"i2u\"][\"min_word_freq\"])}_min_word_freq_gpu.pth.tar'\n",
    "dir_name = config[\"i2u\"][\"dir_name\"]\n",
    "model_params = config[\"i2u\"][\"model_params\"]\n",
    "train_params = config[\"i2u\"][\"train_params\"]\n",
    "data_folder = f'../../data/processed/{dir_name}/'  # folder with data files saved by create_input_files.py\n",
    "# data_name = 'coco_4_cap_per_img_5_min_word_freq'  # base name shared by data files\n",
    "#data_name = f'coco_{str(config[\"i2u\"][\"captions_per_image\"])}_cap_per_img_{str(config[\"i2u\"][\"min_word_freq\"])}_min_word_freq'  # base name shared by data files\n",
    "data_name = f'coco_{str(config[\"i2u\"][\"captions_per_image\"])}_cap_per_img_{str(config[\"i2u\"][\"min_word_freq\"])}_min_word_freq'  # base name shared by data files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(checkpoint, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3072, 1024])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checkpoint[\"model_state_dict\"][\"LM_decoder.layers.0.self_attn.in_proj_weight\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed.weight\n",
      "pos_encoder.pe\n",
      "LM_decoder.layers.0.self_attn.in_proj_weight\n",
      "LM_decoder.layers.0.self_attn.in_proj_bias\n",
      "LM_decoder.layers.0.self_attn.out_proj.weight\n",
      "LM_decoder.layers.0.self_attn.out_proj.bias\n",
      "LM_decoder.layers.0.linear1.weight\n",
      "LM_decoder.layers.0.linear1.bias\n",
      "LM_decoder.layers.0.linear2.weight\n",
      "LM_decoder.layers.0.linear2.bias\n",
      "LM_decoder.layers.0.norm1.weight\n",
      "LM_decoder.layers.0.norm1.bias\n",
      "LM_decoder.layers.0.norm2.weight\n",
      "LM_decoder.layers.0.norm2.bias\n",
      "LM_decoder.layers.1.self_attn.in_proj_weight\n",
      "LM_decoder.layers.1.self_attn.in_proj_bias\n",
      "LM_decoder.layers.1.self_attn.out_proj.weight\n",
      "LM_decoder.layers.1.self_attn.out_proj.bias\n",
      "LM_decoder.layers.1.linear1.weight\n",
      "LM_decoder.layers.1.linear1.bias\n",
      "LM_decoder.layers.1.linear2.weight\n",
      "LM_decoder.layers.1.linear2.bias\n",
      "LM_decoder.layers.1.norm1.weight\n",
      "LM_decoder.layers.1.norm1.bias\n",
      "LM_decoder.layers.1.norm2.weight\n",
      "LM_decoder.layers.1.norm2.bias\n",
      "LM_decoder.layers.2.self_attn.in_proj_weight\n",
      "LM_decoder.layers.2.self_attn.in_proj_bias\n",
      "LM_decoder.layers.2.self_attn.out_proj.weight\n",
      "LM_decoder.layers.2.self_attn.out_proj.bias\n",
      "LM_decoder.layers.2.linear1.weight\n",
      "LM_decoder.layers.2.linear1.bias\n",
      "LM_decoder.layers.2.linear2.weight\n",
      "LM_decoder.layers.2.linear2.bias\n",
      "LM_decoder.layers.2.norm1.weight\n",
      "LM_decoder.layers.2.norm1.bias\n",
      "LM_decoder.layers.2.norm2.weight\n",
      "LM_decoder.layers.2.norm2.bias\n",
      "LM_decoder.layers.3.self_attn.in_proj_weight\n",
      "LM_decoder.layers.3.self_attn.in_proj_bias\n",
      "LM_decoder.layers.3.self_attn.out_proj.weight\n",
      "LM_decoder.layers.3.self_attn.out_proj.bias\n",
      "LM_decoder.layers.3.linear1.weight\n",
      "LM_decoder.layers.3.linear1.bias\n",
      "LM_decoder.layers.3.linear2.weight\n",
      "LM_decoder.layers.3.linear2.bias\n",
      "LM_decoder.layers.3.norm1.weight\n",
      "LM_decoder.layers.3.norm1.bias\n",
      "LM_decoder.layers.3.norm2.weight\n",
      "LM_decoder.layers.3.norm2.bias\n",
      "LM_decoder.layers.4.self_attn.in_proj_weight\n",
      "LM_decoder.layers.4.self_attn.in_proj_bias\n",
      "LM_decoder.layers.4.self_attn.out_proj.weight\n",
      "LM_decoder.layers.4.self_attn.out_proj.bias\n",
      "LM_decoder.layers.4.linear1.weight\n",
      "LM_decoder.layers.4.linear1.bias\n",
      "LM_decoder.layers.4.linear2.weight\n",
      "LM_decoder.layers.4.linear2.bias\n",
      "LM_decoder.layers.4.norm1.weight\n",
      "LM_decoder.layers.4.norm1.bias\n",
      "LM_decoder.layers.4.norm2.weight\n",
      "LM_decoder.layers.4.norm2.bias\n",
      "LM_decoder.layers.5.self_attn.in_proj_weight\n",
      "LM_decoder.layers.5.self_attn.in_proj_bias\n",
      "LM_decoder.layers.5.self_attn.out_proj.weight\n",
      "LM_decoder.layers.5.self_attn.out_proj.bias\n",
      "LM_decoder.layers.5.linear1.weight\n",
      "LM_decoder.layers.5.linear1.bias\n",
      "LM_decoder.layers.5.linear2.weight\n",
      "LM_decoder.layers.5.linear2.bias\n",
      "LM_decoder.layers.5.norm1.weight\n",
      "LM_decoder.layers.5.norm1.bias\n",
      "LM_decoder.layers.5.norm2.weight\n",
      "LM_decoder.layers.5.norm2.bias\n",
      "LM_decoder.norm.weight\n",
      "LM_decoder.norm.bias\n",
      "classifier.weight\n",
      "classifier.bias\n",
      "image_encoder.resnet.0.weight\n",
      "image_encoder.resnet.1.weight\n",
      "image_encoder.resnet.1.bias\n",
      "image_encoder.resnet.1.running_mean\n",
      "image_encoder.resnet.1.running_var\n",
      "image_encoder.resnet.1.num_batches_tracked\n",
      "image_encoder.resnet.4.0.conv1.weight\n",
      "image_encoder.resnet.4.0.bn1.weight\n",
      "image_encoder.resnet.4.0.bn1.bias\n",
      "image_encoder.resnet.4.0.bn1.running_mean\n",
      "image_encoder.resnet.4.0.bn1.running_var\n",
      "image_encoder.resnet.4.0.bn1.num_batches_tracked\n",
      "image_encoder.resnet.4.0.conv2.weight\n",
      "image_encoder.resnet.4.0.bn2.weight\n",
      "image_encoder.resnet.4.0.bn2.bias\n",
      "image_encoder.resnet.4.0.bn2.running_mean\n",
      "image_encoder.resnet.4.0.bn2.running_var\n",
      "image_encoder.resnet.4.0.bn2.num_batches_tracked\n",
      "image_encoder.resnet.4.0.conv3.weight\n",
      "image_encoder.resnet.4.0.bn3.weight\n",
      "image_encoder.resnet.4.0.bn3.bias\n",
      "image_encoder.resnet.4.0.bn3.running_mean\n",
      "image_encoder.resnet.4.0.bn3.running_var\n",
      "image_encoder.resnet.4.0.bn3.num_batches_tracked\n",
      "image_encoder.resnet.4.0.downsample.0.weight\n",
      "image_encoder.resnet.4.0.downsample.1.weight\n",
      "image_encoder.resnet.4.0.downsample.1.bias\n",
      "image_encoder.resnet.4.0.downsample.1.running_mean\n",
      "image_encoder.resnet.4.0.downsample.1.running_var\n",
      "image_encoder.resnet.4.0.downsample.1.num_batches_tracked\n",
      "image_encoder.resnet.4.1.conv1.weight\n",
      "image_encoder.resnet.4.1.bn1.weight\n",
      "image_encoder.resnet.4.1.bn1.bias\n",
      "image_encoder.resnet.4.1.bn1.running_mean\n",
      "image_encoder.resnet.4.1.bn1.running_var\n",
      "image_encoder.resnet.4.1.bn1.num_batches_tracked\n",
      "image_encoder.resnet.4.1.conv2.weight\n",
      "image_encoder.resnet.4.1.bn2.weight\n",
      "image_encoder.resnet.4.1.bn2.bias\n",
      "image_encoder.resnet.4.1.bn2.running_mean\n",
      "image_encoder.resnet.4.1.bn2.running_var\n",
      "image_encoder.resnet.4.1.bn2.num_batches_tracked\n",
      "image_encoder.resnet.4.1.conv3.weight\n",
      "image_encoder.resnet.4.1.bn3.weight\n",
      "image_encoder.resnet.4.1.bn3.bias\n",
      "image_encoder.resnet.4.1.bn3.running_mean\n",
      "image_encoder.resnet.4.1.bn3.running_var\n",
      "image_encoder.resnet.4.1.bn3.num_batches_tracked\n",
      "image_encoder.resnet.4.2.conv1.weight\n",
      "image_encoder.resnet.4.2.bn1.weight\n",
      "image_encoder.resnet.4.2.bn1.bias\n",
      "image_encoder.resnet.4.2.bn1.running_mean\n",
      "image_encoder.resnet.4.2.bn1.running_var\n",
      "image_encoder.resnet.4.2.bn1.num_batches_tracked\n",
      "image_encoder.resnet.4.2.conv2.weight\n",
      "image_encoder.resnet.4.2.bn2.weight\n",
      "image_encoder.resnet.4.2.bn2.bias\n",
      "image_encoder.resnet.4.2.bn2.running_mean\n",
      "image_encoder.resnet.4.2.bn2.running_var\n",
      "image_encoder.resnet.4.2.bn2.num_batches_tracked\n",
      "image_encoder.resnet.4.2.conv3.weight\n",
      "image_encoder.resnet.4.2.bn3.weight\n",
      "image_encoder.resnet.4.2.bn3.bias\n",
      "image_encoder.resnet.4.2.bn3.running_mean\n",
      "image_encoder.resnet.4.2.bn3.running_var\n",
      "image_encoder.resnet.4.2.bn3.num_batches_tracked\n",
      "image_encoder.resnet.5.0.conv1.weight\n",
      "image_encoder.resnet.5.0.bn1.weight\n",
      "image_encoder.resnet.5.0.bn1.bias\n",
      "image_encoder.resnet.5.0.bn1.running_mean\n",
      "image_encoder.resnet.5.0.bn1.running_var\n",
      "image_encoder.resnet.5.0.bn1.num_batches_tracked\n",
      "image_encoder.resnet.5.0.conv2.weight\n",
      "image_encoder.resnet.5.0.bn2.weight\n",
      "image_encoder.resnet.5.0.bn2.bias\n",
      "image_encoder.resnet.5.0.bn2.running_mean\n",
      "image_encoder.resnet.5.0.bn2.running_var\n",
      "image_encoder.resnet.5.0.bn2.num_batches_tracked\n",
      "image_encoder.resnet.5.0.conv3.weight\n",
      "image_encoder.resnet.5.0.bn3.weight\n",
      "image_encoder.resnet.5.0.bn3.bias\n",
      "image_encoder.resnet.5.0.bn3.running_mean\n",
      "image_encoder.resnet.5.0.bn3.running_var\n",
      "image_encoder.resnet.5.0.bn3.num_batches_tracked\n",
      "image_encoder.resnet.5.0.downsample.0.weight\n",
      "image_encoder.resnet.5.0.downsample.1.weight\n",
      "image_encoder.resnet.5.0.downsample.1.bias\n",
      "image_encoder.resnet.5.0.downsample.1.running_mean\n",
      "image_encoder.resnet.5.0.downsample.1.running_var\n",
      "image_encoder.resnet.5.0.downsample.1.num_batches_tracked\n",
      "image_encoder.resnet.5.1.conv1.weight\n",
      "image_encoder.resnet.5.1.bn1.weight\n",
      "image_encoder.resnet.5.1.bn1.bias\n",
      "image_encoder.resnet.5.1.bn1.running_mean\n",
      "image_encoder.resnet.5.1.bn1.running_var\n",
      "image_encoder.resnet.5.1.bn1.num_batches_tracked\n",
      "image_encoder.resnet.5.1.conv2.weight\n",
      "image_encoder.resnet.5.1.bn2.weight\n",
      "image_encoder.resnet.5.1.bn2.bias\n",
      "image_encoder.resnet.5.1.bn2.running_mean\n",
      "image_encoder.resnet.5.1.bn2.running_var\n",
      "image_encoder.resnet.5.1.bn2.num_batches_tracked\n",
      "image_encoder.resnet.5.1.conv3.weight\n",
      "image_encoder.resnet.5.1.bn3.weight\n",
      "image_encoder.resnet.5.1.bn3.bias\n",
      "image_encoder.resnet.5.1.bn3.running_mean\n",
      "image_encoder.resnet.5.1.bn3.running_var\n",
      "image_encoder.resnet.5.1.bn3.num_batches_tracked\n",
      "image_encoder.resnet.5.2.conv1.weight\n",
      "image_encoder.resnet.5.2.bn1.weight\n",
      "image_encoder.resnet.5.2.bn1.bias\n",
      "image_encoder.resnet.5.2.bn1.running_mean\n",
      "image_encoder.resnet.5.2.bn1.running_var\n",
      "image_encoder.resnet.5.2.bn1.num_batches_tracked\n",
      "image_encoder.resnet.5.2.conv2.weight\n",
      "image_encoder.resnet.5.2.bn2.weight\n",
      "image_encoder.resnet.5.2.bn2.bias\n",
      "image_encoder.resnet.5.2.bn2.running_mean\n",
      "image_encoder.resnet.5.2.bn2.running_var\n",
      "image_encoder.resnet.5.2.bn2.num_batches_tracked\n",
      "image_encoder.resnet.5.2.conv3.weight\n",
      "image_encoder.resnet.5.2.bn3.weight\n",
      "image_encoder.resnet.5.2.bn3.bias\n",
      "image_encoder.resnet.5.2.bn3.running_mean\n",
      "image_encoder.resnet.5.2.bn3.running_var\n",
      "image_encoder.resnet.5.2.bn3.num_batches_tracked\n",
      "image_encoder.resnet.5.3.conv1.weight\n",
      "image_encoder.resnet.5.3.bn1.weight\n",
      "image_encoder.resnet.5.3.bn1.bias\n",
      "image_encoder.resnet.5.3.bn1.running_mean\n",
      "image_encoder.resnet.5.3.bn1.running_var\n",
      "image_encoder.resnet.5.3.bn1.num_batches_tracked\n",
      "image_encoder.resnet.5.3.conv2.weight\n",
      "image_encoder.resnet.5.3.bn2.weight\n",
      "image_encoder.resnet.5.3.bn2.bias\n",
      "image_encoder.resnet.5.3.bn2.running_mean\n",
      "image_encoder.resnet.5.3.bn2.running_var\n",
      "image_encoder.resnet.5.3.bn2.num_batches_tracked\n",
      "image_encoder.resnet.5.3.conv3.weight\n",
      "image_encoder.resnet.5.3.bn3.weight\n",
      "image_encoder.resnet.5.3.bn3.bias\n",
      "image_encoder.resnet.5.3.bn3.running_mean\n",
      "image_encoder.resnet.5.3.bn3.running_var\n",
      "image_encoder.resnet.5.3.bn3.num_batches_tracked\n",
      "image_encoder.resnet.6.0.conv1.weight\n",
      "image_encoder.resnet.6.0.bn1.weight\n",
      "image_encoder.resnet.6.0.bn1.bias\n",
      "image_encoder.resnet.6.0.bn1.running_mean\n",
      "image_encoder.resnet.6.0.bn1.running_var\n",
      "image_encoder.resnet.6.0.bn1.num_batches_tracked\n",
      "image_encoder.resnet.6.0.conv2.weight\n",
      "image_encoder.resnet.6.0.bn2.weight\n",
      "image_encoder.resnet.6.0.bn2.bias\n",
      "image_encoder.resnet.6.0.bn2.running_mean\n",
      "image_encoder.resnet.6.0.bn2.running_var\n",
      "image_encoder.resnet.6.0.bn2.num_batches_tracked\n",
      "image_encoder.resnet.6.0.conv3.weight\n",
      "image_encoder.resnet.6.0.bn3.weight\n",
      "image_encoder.resnet.6.0.bn3.bias\n",
      "image_encoder.resnet.6.0.bn3.running_mean\n",
      "image_encoder.resnet.6.0.bn3.running_var\n",
      "image_encoder.resnet.6.0.bn3.num_batches_tracked\n",
      "image_encoder.resnet.6.0.downsample.0.weight\n",
      "image_encoder.resnet.6.0.downsample.1.weight\n",
      "image_encoder.resnet.6.0.downsample.1.bias\n",
      "image_encoder.resnet.6.0.downsample.1.running_mean\n",
      "image_encoder.resnet.6.0.downsample.1.running_var\n",
      "image_encoder.resnet.6.0.downsample.1.num_batches_tracked\n",
      "image_encoder.resnet.6.1.conv1.weight\n",
      "image_encoder.resnet.6.1.bn1.weight\n",
      "image_encoder.resnet.6.1.bn1.bias\n",
      "image_encoder.resnet.6.1.bn1.running_mean\n",
      "image_encoder.resnet.6.1.bn1.running_var\n",
      "image_encoder.resnet.6.1.bn1.num_batches_tracked\n",
      "image_encoder.resnet.6.1.conv2.weight\n",
      "image_encoder.resnet.6.1.bn2.weight\n",
      "image_encoder.resnet.6.1.bn2.bias\n",
      "image_encoder.resnet.6.1.bn2.running_mean\n",
      "image_encoder.resnet.6.1.bn2.running_var\n",
      "image_encoder.resnet.6.1.bn2.num_batches_tracked\n",
      "image_encoder.resnet.6.1.conv3.weight\n",
      "image_encoder.resnet.6.1.bn3.weight\n",
      "image_encoder.resnet.6.1.bn3.bias\n",
      "image_encoder.resnet.6.1.bn3.running_mean\n",
      "image_encoder.resnet.6.1.bn3.running_var\n",
      "image_encoder.resnet.6.1.bn3.num_batches_tracked\n",
      "image_encoder.resnet.6.2.conv1.weight\n",
      "image_encoder.resnet.6.2.bn1.weight\n",
      "image_encoder.resnet.6.2.bn1.bias\n",
      "image_encoder.resnet.6.2.bn1.running_mean\n",
      "image_encoder.resnet.6.2.bn1.running_var\n",
      "image_encoder.resnet.6.2.bn1.num_batches_tracked\n",
      "image_encoder.resnet.6.2.conv2.weight\n",
      "image_encoder.resnet.6.2.bn2.weight\n",
      "image_encoder.resnet.6.2.bn2.bias\n",
      "image_encoder.resnet.6.2.bn2.running_mean\n",
      "image_encoder.resnet.6.2.bn2.running_var\n",
      "image_encoder.resnet.6.2.bn2.num_batches_tracked\n",
      "image_encoder.resnet.6.2.conv3.weight\n",
      "image_encoder.resnet.6.2.bn3.weight\n",
      "image_encoder.resnet.6.2.bn3.bias\n",
      "image_encoder.resnet.6.2.bn3.running_mean\n",
      "image_encoder.resnet.6.2.bn3.running_var\n",
      "image_encoder.resnet.6.2.bn3.num_batches_tracked\n",
      "image_encoder.resnet.6.3.conv1.weight\n",
      "image_encoder.resnet.6.3.bn1.weight\n",
      "image_encoder.resnet.6.3.bn1.bias\n",
      "image_encoder.resnet.6.3.bn1.running_mean\n",
      "image_encoder.resnet.6.3.bn1.running_var\n",
      "image_encoder.resnet.6.3.bn1.num_batches_tracked\n",
      "image_encoder.resnet.6.3.conv2.weight\n",
      "image_encoder.resnet.6.3.bn2.weight\n",
      "image_encoder.resnet.6.3.bn2.bias\n",
      "image_encoder.resnet.6.3.bn2.running_mean\n",
      "image_encoder.resnet.6.3.bn2.running_var\n",
      "image_encoder.resnet.6.3.bn2.num_batches_tracked\n",
      "image_encoder.resnet.6.3.conv3.weight\n",
      "image_encoder.resnet.6.3.bn3.weight\n",
      "image_encoder.resnet.6.3.bn3.bias\n",
      "image_encoder.resnet.6.3.bn3.running_mean\n",
      "image_encoder.resnet.6.3.bn3.running_var\n",
      "image_encoder.resnet.6.3.bn3.num_batches_tracked\n",
      "image_encoder.resnet.6.4.conv1.weight\n",
      "image_encoder.resnet.6.4.bn1.weight\n",
      "image_encoder.resnet.6.4.bn1.bias\n",
      "image_encoder.resnet.6.4.bn1.running_mean\n",
      "image_encoder.resnet.6.4.bn1.running_var\n",
      "image_encoder.resnet.6.4.bn1.num_batches_tracked\n",
      "image_encoder.resnet.6.4.conv2.weight\n",
      "image_encoder.resnet.6.4.bn2.weight\n",
      "image_encoder.resnet.6.4.bn2.bias\n",
      "image_encoder.resnet.6.4.bn2.running_mean\n",
      "image_encoder.resnet.6.4.bn2.running_var\n",
      "image_encoder.resnet.6.4.bn2.num_batches_tracked\n",
      "image_encoder.resnet.6.4.conv3.weight\n",
      "image_encoder.resnet.6.4.bn3.weight\n",
      "image_encoder.resnet.6.4.bn3.bias\n",
      "image_encoder.resnet.6.4.bn3.running_mean\n",
      "image_encoder.resnet.6.4.bn3.running_var\n",
      "image_encoder.resnet.6.4.bn3.num_batches_tracked\n",
      "image_encoder.resnet.6.5.conv1.weight\n",
      "image_encoder.resnet.6.5.bn1.weight\n",
      "image_encoder.resnet.6.5.bn1.bias\n",
      "image_encoder.resnet.6.5.bn1.running_mean\n",
      "image_encoder.resnet.6.5.bn1.running_var\n",
      "image_encoder.resnet.6.5.bn1.num_batches_tracked\n",
      "image_encoder.resnet.6.5.conv2.weight\n",
      "image_encoder.resnet.6.5.bn2.weight\n",
      "image_encoder.resnet.6.5.bn2.bias\n",
      "image_encoder.resnet.6.5.bn2.running_mean\n",
      "image_encoder.resnet.6.5.bn2.running_var\n",
      "image_encoder.resnet.6.5.bn2.num_batches_tracked\n",
      "image_encoder.resnet.6.5.conv3.weight\n",
      "image_encoder.resnet.6.5.bn3.weight\n",
      "image_encoder.resnet.6.5.bn3.bias\n",
      "image_encoder.resnet.6.5.bn3.running_mean\n",
      "image_encoder.resnet.6.5.bn3.running_var\n",
      "image_encoder.resnet.6.5.bn3.num_batches_tracked\n",
      "image_encoder.resnet.7.0.conv1.weight\n",
      "image_encoder.resnet.7.0.bn1.weight\n",
      "image_encoder.resnet.7.0.bn1.bias\n",
      "image_encoder.resnet.7.0.bn1.running_mean\n",
      "image_encoder.resnet.7.0.bn1.running_var\n",
      "image_encoder.resnet.7.0.bn1.num_batches_tracked\n",
      "image_encoder.resnet.7.0.conv2.weight\n",
      "image_encoder.resnet.7.0.bn2.weight\n",
      "image_encoder.resnet.7.0.bn2.bias\n",
      "image_encoder.resnet.7.0.bn2.running_mean\n",
      "image_encoder.resnet.7.0.bn2.running_var\n",
      "image_encoder.resnet.7.0.bn2.num_batches_tracked\n",
      "image_encoder.resnet.7.0.conv3.weight\n",
      "image_encoder.resnet.7.0.bn3.weight\n",
      "image_encoder.resnet.7.0.bn3.bias\n",
      "image_encoder.resnet.7.0.bn3.running_mean\n",
      "image_encoder.resnet.7.0.bn3.running_var\n",
      "image_encoder.resnet.7.0.bn3.num_batches_tracked\n",
      "image_encoder.resnet.7.0.downsample.0.weight\n",
      "image_encoder.resnet.7.0.downsample.1.weight\n",
      "image_encoder.resnet.7.0.downsample.1.bias\n",
      "image_encoder.resnet.7.0.downsample.1.running_mean\n",
      "image_encoder.resnet.7.0.downsample.1.running_var\n",
      "image_encoder.resnet.7.0.downsample.1.num_batches_tracked\n",
      "image_encoder.resnet.7.1.conv1.weight\n",
      "image_encoder.resnet.7.1.bn1.weight\n",
      "image_encoder.resnet.7.1.bn1.bias\n",
      "image_encoder.resnet.7.1.bn1.running_mean\n",
      "image_encoder.resnet.7.1.bn1.running_var\n",
      "image_encoder.resnet.7.1.bn1.num_batches_tracked\n",
      "image_encoder.resnet.7.1.conv2.weight\n",
      "image_encoder.resnet.7.1.bn2.weight\n",
      "image_encoder.resnet.7.1.bn2.bias\n",
      "image_encoder.resnet.7.1.bn2.running_mean\n",
      "image_encoder.resnet.7.1.bn2.running_var\n",
      "image_encoder.resnet.7.1.bn2.num_batches_tracked\n",
      "image_encoder.resnet.7.1.conv3.weight\n",
      "image_encoder.resnet.7.1.bn3.weight\n",
      "image_encoder.resnet.7.1.bn3.bias\n",
      "image_encoder.resnet.7.1.bn3.running_mean\n",
      "image_encoder.resnet.7.1.bn3.running_var\n",
      "image_encoder.resnet.7.1.bn3.num_batches_tracked\n",
      "image_encoder.resnet.7.2.conv1.weight\n",
      "image_encoder.resnet.7.2.bn1.weight\n",
      "image_encoder.resnet.7.2.bn1.bias\n",
      "image_encoder.resnet.7.2.bn1.running_mean\n",
      "image_encoder.resnet.7.2.bn1.running_var\n",
      "image_encoder.resnet.7.2.bn1.num_batches_tracked\n",
      "image_encoder.resnet.7.2.conv2.weight\n",
      "image_encoder.resnet.7.2.bn2.weight\n",
      "image_encoder.resnet.7.2.bn2.bias\n",
      "image_encoder.resnet.7.2.bn2.running_mean\n",
      "image_encoder.resnet.7.2.bn2.running_var\n",
      "image_encoder.resnet.7.2.bn2.num_batches_tracked\n",
      "image_encoder.resnet.7.2.conv3.weight\n",
      "image_encoder.resnet.7.2.bn3.weight\n",
      "image_encoder.resnet.7.2.bn3.bias\n",
      "image_encoder.resnet.7.2.bn3.running_mean\n",
      "image_encoder.resnet.7.2.bn3.running_var\n",
      "image_encoder.resnet.7.2.bn3.num_batches_tracked\n",
      "image_encoder.to_embedding.weight\n",
      "image_encoder.to_embedding.bias\n",
      "refine_encoder.layers.0.encoder_attn.relative_position_bias_table\n",
      "refine_encoder.layers.0.encoder_attn.relative_position_index\n",
      "refine_encoder.layers.0.encoder_attn.q_linear.weight\n",
      "refine_encoder.layers.0.encoder_attn.q_linear.bias\n",
      "refine_encoder.layers.0.encoder_attn.k_linear.weight\n",
      "refine_encoder.layers.0.encoder_attn.k_linear.bias\n",
      "refine_encoder.layers.0.encoder_attn.v_linear.weight\n",
      "refine_encoder.layers.0.encoder_attn.v_linear.bias\n",
      "refine_encoder.layers.0.encoder_attn.o_linear.weight\n",
      "refine_encoder.layers.0.encoder_attn.o_linear.bias\n",
      "refine_encoder.layers.0.layer_norm1.weight\n",
      "refine_encoder.layers.0.layer_norm1.bias\n",
      "refine_encoder.layers.0.ff_layer.fc1.weight\n",
      "refine_encoder.layers.0.ff_layer.fc1.bias\n",
      "refine_encoder.layers.0.ff_layer.fc2.weight\n",
      "refine_encoder.layers.0.ff_layer.fc2.bias\n",
      "refine_encoder.layers.0.layer_norm2.weight\n",
      "refine_encoder.layers.0.layer_norm2.bias\n",
      "refine_encoder.layers.1.encoder_attn.relative_position_bias_table\n",
      "refine_encoder.layers.1.encoder_attn.relative_position_index\n",
      "refine_encoder.layers.1.encoder_attn.q_linear.weight\n",
      "refine_encoder.layers.1.encoder_attn.q_linear.bias\n",
      "refine_encoder.layers.1.encoder_attn.k_linear.weight\n",
      "refine_encoder.layers.1.encoder_attn.k_linear.bias\n",
      "refine_encoder.layers.1.encoder_attn.v_linear.weight\n",
      "refine_encoder.layers.1.encoder_attn.v_linear.bias\n",
      "refine_encoder.layers.1.encoder_attn.o_linear.weight\n",
      "refine_encoder.layers.1.encoder_attn.o_linear.bias\n",
      "refine_encoder.layers.1.layer_norm1.weight\n",
      "refine_encoder.layers.1.layer_norm1.bias\n",
      "refine_encoder.layers.1.ff_layer.fc1.weight\n",
      "refine_encoder.layers.1.ff_layer.fc1.bias\n",
      "refine_encoder.layers.1.ff_layer.fc2.weight\n",
      "refine_encoder.layers.1.ff_layer.fc2.bias\n",
      "refine_encoder.layers.1.layer_norm2.weight\n",
      "refine_encoder.layers.1.layer_norm2.bias\n",
      "refine_encoder.layers.2.encoder_attn.relative_position_bias_table\n",
      "refine_encoder.layers.2.encoder_attn.relative_position_index\n",
      "refine_encoder.layers.2.encoder_attn.q_linear.weight\n",
      "refine_encoder.layers.2.encoder_attn.q_linear.bias\n",
      "refine_encoder.layers.2.encoder_attn.k_linear.weight\n",
      "refine_encoder.layers.2.encoder_attn.k_linear.bias\n",
      "refine_encoder.layers.2.encoder_attn.v_linear.weight\n",
      "refine_encoder.layers.2.encoder_attn.v_linear.bias\n",
      "refine_encoder.layers.2.encoder_attn.o_linear.weight\n",
      "refine_encoder.layers.2.encoder_attn.o_linear.bias\n",
      "refine_encoder.layers.2.layer_norm1.weight\n",
      "refine_encoder.layers.2.layer_norm1.bias\n",
      "refine_encoder.layers.2.ff_layer.fc1.weight\n",
      "refine_encoder.layers.2.ff_layer.fc1.bias\n",
      "refine_encoder.layers.2.ff_layer.fc2.weight\n",
      "refine_encoder.layers.2.ff_layer.fc2.bias\n",
      "refine_encoder.layers.2.layer_norm2.weight\n",
      "refine_encoder.layers.2.layer_norm2.bias\n",
      "prefusion_layer.0.weight\n",
      "prefusion_layer.0.bias\n",
      "prefusion_norm.weight\n",
      "prefusion_norm.bias\n",
      "decoder.layers.0.self_attn.in_proj_weight\n",
      "decoder.layers.0.self_attn.in_proj_bias\n",
      "decoder.layers.0.self_attn.out_proj.weight\n",
      "decoder.layers.0.self_attn.out_proj.bias\n",
      "decoder.layers.0.multihead_attn.in_proj_weight\n",
      "decoder.layers.0.multihead_attn.in_proj_bias\n",
      "decoder.layers.0.multihead_attn.out_proj.weight\n",
      "decoder.layers.0.multihead_attn.out_proj.bias\n",
      "decoder.layers.0.linear1.weight\n",
      "decoder.layers.0.linear1.bias\n",
      "decoder.layers.0.linear2.weight\n",
      "decoder.layers.0.linear2.bias\n",
      "decoder.layers.0.norm1.weight\n",
      "decoder.layers.0.norm1.bias\n",
      "decoder.layers.0.norm2.weight\n",
      "decoder.layers.0.norm2.bias\n",
      "decoder.layers.0.norm3.weight\n",
      "decoder.layers.0.norm3.bias\n",
      "decoder.layers.1.self_attn.in_proj_weight\n",
      "decoder.layers.1.self_attn.in_proj_bias\n",
      "decoder.layers.1.self_attn.out_proj.weight\n",
      "decoder.layers.1.self_attn.out_proj.bias\n",
      "decoder.layers.1.multihead_attn.in_proj_weight\n",
      "decoder.layers.1.multihead_attn.in_proj_bias\n",
      "decoder.layers.1.multihead_attn.out_proj.weight\n",
      "decoder.layers.1.multihead_attn.out_proj.bias\n",
      "decoder.layers.1.linear1.weight\n",
      "decoder.layers.1.linear1.bias\n",
      "decoder.layers.1.linear2.weight\n",
      "decoder.layers.1.linear2.bias\n",
      "decoder.layers.1.norm1.weight\n",
      "decoder.layers.1.norm1.bias\n",
      "decoder.layers.1.norm2.weight\n",
      "decoder.layers.1.norm2.bias\n",
      "decoder.layers.1.norm3.weight\n",
      "decoder.layers.1.norm3.bias\n",
      "decoder.layers.2.self_attn.in_proj_weight\n",
      "decoder.layers.2.self_attn.in_proj_bias\n",
      "decoder.layers.2.self_attn.out_proj.weight\n",
      "decoder.layers.2.self_attn.out_proj.bias\n",
      "decoder.layers.2.multihead_attn.in_proj_weight\n",
      "decoder.layers.2.multihead_attn.in_proj_bias\n",
      "decoder.layers.2.multihead_attn.out_proj.weight\n",
      "decoder.layers.2.multihead_attn.out_proj.bias\n",
      "decoder.layers.2.linear1.weight\n",
      "decoder.layers.2.linear1.bias\n",
      "decoder.layers.2.linear2.weight\n",
      "decoder.layers.2.linear2.bias\n",
      "decoder.layers.2.norm1.weight\n",
      "decoder.layers.2.norm1.bias\n",
      "decoder.layers.2.norm2.weight\n",
      "decoder.layers.2.norm2.bias\n",
      "decoder.layers.2.norm3.weight\n",
      "decoder.layers.2.norm3.bias\n",
      "decoder.layers.3.self_attn.in_proj_weight\n",
      "decoder.layers.3.self_attn.in_proj_bias\n",
      "decoder.layers.3.self_attn.out_proj.weight\n",
      "decoder.layers.3.self_attn.out_proj.bias\n",
      "decoder.layers.3.multihead_attn.in_proj_weight\n",
      "decoder.layers.3.multihead_attn.in_proj_bias\n",
      "decoder.layers.3.multihead_attn.out_proj.weight\n",
      "decoder.layers.3.multihead_attn.out_proj.bias\n",
      "decoder.layers.3.linear1.weight\n",
      "decoder.layers.3.linear1.bias\n",
      "decoder.layers.3.linear2.weight\n",
      "decoder.layers.3.linear2.bias\n",
      "decoder.layers.3.norm1.weight\n",
      "decoder.layers.3.norm1.bias\n",
      "decoder.layers.3.norm2.weight\n",
      "decoder.layers.3.norm2.bias\n",
      "decoder.layers.3.norm3.weight\n",
      "decoder.layers.3.norm3.bias\n",
      "decoder.layers.4.self_attn.in_proj_weight\n",
      "decoder.layers.4.self_attn.in_proj_bias\n",
      "decoder.layers.4.self_attn.out_proj.weight\n",
      "decoder.layers.4.self_attn.out_proj.bias\n",
      "decoder.layers.4.multihead_attn.in_proj_weight\n",
      "decoder.layers.4.multihead_attn.in_proj_bias\n",
      "decoder.layers.4.multihead_attn.out_proj.weight\n",
      "decoder.layers.4.multihead_attn.out_proj.bias\n",
      "decoder.layers.4.linear1.weight\n",
      "decoder.layers.4.linear1.bias\n",
      "decoder.layers.4.linear2.weight\n",
      "decoder.layers.4.linear2.bias\n",
      "decoder.layers.4.norm1.weight\n",
      "decoder.layers.4.norm1.bias\n",
      "decoder.layers.4.norm2.weight\n",
      "decoder.layers.4.norm2.bias\n",
      "decoder.layers.4.norm3.weight\n",
      "decoder.layers.4.norm3.bias\n",
      "decoder.layers.5.self_attn.in_proj_weight\n",
      "decoder.layers.5.self_attn.in_proj_bias\n",
      "decoder.layers.5.self_attn.out_proj.weight\n",
      "decoder.layers.5.self_attn.out_proj.bias\n",
      "decoder.layers.5.multihead_attn.in_proj_weight\n",
      "decoder.layers.5.multihead_attn.in_proj_bias\n",
      "decoder.layers.5.multihead_attn.out_proj.weight\n",
      "decoder.layers.5.multihead_attn.out_proj.bias\n",
      "decoder.layers.5.linear1.weight\n",
      "decoder.layers.5.linear1.bias\n",
      "decoder.layers.5.linear2.weight\n",
      "decoder.layers.5.linear2.bias\n",
      "decoder.layers.5.norm1.weight\n",
      "decoder.layers.5.norm1.bias\n",
      "decoder.layers.5.norm2.weight\n",
      "decoder.layers.5.norm2.bias\n",
      "decoder.layers.5.norm3.weight\n",
      "decoder.layers.5.norm3.bias\n",
      "decoder.norm.weight\n",
      "decoder.norm.bias\n"
     ]
    }
   ],
   "source": [
    "for k,v in checkpoint[\"model_state_dict\"].items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.self_attn.in_proj_weight torch.Size([3072, 1024])\n",
      "layers.0.self_attn.in_proj_bias torch.Size([3072])\n",
      "layers.0.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "layers.0.self_attn.out_proj.bias torch.Size([1024])\n",
      "layers.0.linear1.weight torch.Size([4096, 1024])\n",
      "layers.0.linear1.bias torch.Size([4096])\n",
      "layers.0.linear2.weight torch.Size([1024, 4096])\n",
      "layers.0.linear2.bias torch.Size([1024])\n",
      "layers.0.norm1.weight torch.Size([1024])\n",
      "layers.0.norm1.bias torch.Size([1024])\n",
      "layers.0.norm2.weight torch.Size([1024])\n",
      "layers.0.norm2.bias torch.Size([1024])\n",
      "layers.1.self_attn.in_proj_weight torch.Size([3072, 1024])\n",
      "layers.1.self_attn.in_proj_bias torch.Size([3072])\n",
      "layers.1.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "layers.1.self_attn.out_proj.bias torch.Size([1024])\n",
      "layers.1.linear1.weight torch.Size([4096, 1024])\n",
      "layers.1.linear1.bias torch.Size([4096])\n",
      "layers.1.linear2.weight torch.Size([1024, 4096])\n",
      "layers.1.linear2.bias torch.Size([1024])\n",
      "layers.1.norm1.weight torch.Size([1024])\n",
      "layers.1.norm1.bias torch.Size([1024])\n",
      "layers.1.norm2.weight torch.Size([1024])\n",
      "layers.1.norm2.bias torch.Size([1024])\n",
      "layers.2.self_attn.in_proj_weight torch.Size([3072, 1024])\n",
      "layers.2.self_attn.in_proj_bias torch.Size([3072])\n",
      "layers.2.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "layers.2.self_attn.out_proj.bias torch.Size([1024])\n",
      "layers.2.linear1.weight torch.Size([4096, 1024])\n",
      "layers.2.linear1.bias torch.Size([4096])\n",
      "layers.2.linear2.weight torch.Size([1024, 4096])\n",
      "layers.2.linear2.bias torch.Size([1024])\n",
      "layers.2.norm1.weight torch.Size([1024])\n",
      "layers.2.norm1.bias torch.Size([1024])\n",
      "layers.2.norm2.weight torch.Size([1024])\n",
      "layers.2.norm2.bias torch.Size([1024])\n",
      "layers.3.self_attn.in_proj_weight torch.Size([3072, 1024])\n",
      "layers.3.self_attn.in_proj_bias torch.Size([3072])\n",
      "layers.3.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "layers.3.self_attn.out_proj.bias torch.Size([1024])\n",
      "layers.3.linear1.weight torch.Size([4096, 1024])\n",
      "layers.3.linear1.bias torch.Size([4096])\n",
      "layers.3.linear2.weight torch.Size([1024, 4096])\n",
      "layers.3.linear2.bias torch.Size([1024])\n",
      "layers.3.norm1.weight torch.Size([1024])\n",
      "layers.3.norm1.bias torch.Size([1024])\n",
      "layers.3.norm2.weight torch.Size([1024])\n",
      "layers.3.norm2.bias torch.Size([1024])\n",
      "layers.4.self_attn.in_proj_weight torch.Size([3072, 1024])\n",
      "layers.4.self_attn.in_proj_bias torch.Size([3072])\n",
      "layers.4.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "layers.4.self_attn.out_proj.bias torch.Size([1024])\n",
      "layers.4.linear1.weight torch.Size([4096, 1024])\n",
      "layers.4.linear1.bias torch.Size([4096])\n",
      "layers.4.linear2.weight torch.Size([1024, 4096])\n",
      "layers.4.linear2.bias torch.Size([1024])\n",
      "layers.4.norm1.weight torch.Size([1024])\n",
      "layers.4.norm1.bias torch.Size([1024])\n",
      "layers.4.norm2.weight torch.Size([1024])\n",
      "layers.4.norm2.bias torch.Size([1024])\n",
      "layers.5.self_attn.in_proj_weight torch.Size([3072, 1024])\n",
      "layers.5.self_attn.in_proj_bias torch.Size([3072])\n",
      "layers.5.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "layers.5.self_attn.out_proj.bias torch.Size([1024])\n",
      "layers.5.linear1.weight torch.Size([4096, 1024])\n",
      "layers.5.linear1.bias torch.Size([4096])\n",
      "layers.5.linear2.weight torch.Size([1024, 4096])\n",
      "layers.5.linear2.bias torch.Size([1024])\n",
      "layers.5.norm1.weight torch.Size([1024])\n",
      "layers.5.norm1.bias torch.Size([1024])\n",
      "layers.5.norm2.weight torch.Size([1024])\n",
      "layers.5.norm2.bias torch.Size([1024])\n",
      "norm.weight torch.Size([1024])\n",
      "norm.bias torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "for key, value in model.LM_decoder.state_dict().items():\n",
    "    #for p in layer.modules():\n",
    "    print(key, value.shape)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.self_attn.in_proj_weight torch.Size([3072, 1024])\n",
      "layers.0.self_attn.in_proj_bias torch.Size([3072])\n",
      "layers.0.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "layers.0.self_attn.out_proj.bias torch.Size([1024])\n",
      "layers.0.multihead_attn.in_proj_weight torch.Size([3072, 1024])\n",
      "layers.0.multihead_attn.in_proj_bias torch.Size([3072])\n",
      "layers.0.multihead_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "layers.0.multihead_attn.out_proj.bias torch.Size([1024])\n",
      "layers.0.linear1.weight torch.Size([4096, 1024])\n",
      "layers.0.linear1.bias torch.Size([4096])\n",
      "layers.0.linear2.weight torch.Size([1024, 4096])\n",
      "layers.0.linear2.bias torch.Size([1024])\n",
      "layers.0.norm1.weight torch.Size([1024])\n",
      "layers.0.norm1.bias torch.Size([1024])\n",
      "layers.0.norm2.weight torch.Size([1024])\n",
      "layers.0.norm2.bias torch.Size([1024])\n",
      "layers.0.norm3.weight torch.Size([1024])\n",
      "layers.0.norm3.bias torch.Size([1024])\n",
      "layers.1.self_attn.in_proj_weight torch.Size([3072, 1024])\n",
      "layers.1.self_attn.in_proj_bias torch.Size([3072])\n",
      "layers.1.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "layers.1.self_attn.out_proj.bias torch.Size([1024])\n",
      "layers.1.multihead_attn.in_proj_weight torch.Size([3072, 1024])\n",
      "layers.1.multihead_attn.in_proj_bias torch.Size([3072])\n",
      "layers.1.multihead_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "layers.1.multihead_attn.out_proj.bias torch.Size([1024])\n",
      "layers.1.linear1.weight torch.Size([4096, 1024])\n",
      "layers.1.linear1.bias torch.Size([4096])\n",
      "layers.1.linear2.weight torch.Size([1024, 4096])\n",
      "layers.1.linear2.bias torch.Size([1024])\n",
      "layers.1.norm1.weight torch.Size([1024])\n",
      "layers.1.norm1.bias torch.Size([1024])\n",
      "layers.1.norm2.weight torch.Size([1024])\n",
      "layers.1.norm2.bias torch.Size([1024])\n",
      "layers.1.norm3.weight torch.Size([1024])\n",
      "layers.1.norm3.bias torch.Size([1024])\n",
      "layers.2.self_attn.in_proj_weight torch.Size([3072, 1024])\n",
      "layers.2.self_attn.in_proj_bias torch.Size([3072])\n",
      "layers.2.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "layers.2.self_attn.out_proj.bias torch.Size([1024])\n",
      "layers.2.multihead_attn.in_proj_weight torch.Size([3072, 1024])\n",
      "layers.2.multihead_attn.in_proj_bias torch.Size([3072])\n",
      "layers.2.multihead_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "layers.2.multihead_attn.out_proj.bias torch.Size([1024])\n",
      "layers.2.linear1.weight torch.Size([4096, 1024])\n",
      "layers.2.linear1.bias torch.Size([4096])\n",
      "layers.2.linear2.weight torch.Size([1024, 4096])\n",
      "layers.2.linear2.bias torch.Size([1024])\n",
      "layers.2.norm1.weight torch.Size([1024])\n",
      "layers.2.norm1.bias torch.Size([1024])\n",
      "layers.2.norm2.weight torch.Size([1024])\n",
      "layers.2.norm2.bias torch.Size([1024])\n",
      "layers.2.norm3.weight torch.Size([1024])\n",
      "layers.2.norm3.bias torch.Size([1024])\n",
      "layers.3.self_attn.in_proj_weight torch.Size([3072, 1024])\n",
      "layers.3.self_attn.in_proj_bias torch.Size([3072])\n",
      "layers.3.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "layers.3.self_attn.out_proj.bias torch.Size([1024])\n",
      "layers.3.multihead_attn.in_proj_weight torch.Size([3072, 1024])\n",
      "layers.3.multihead_attn.in_proj_bias torch.Size([3072])\n",
      "layers.3.multihead_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "layers.3.multihead_attn.out_proj.bias torch.Size([1024])\n",
      "layers.3.linear1.weight torch.Size([4096, 1024])\n",
      "layers.3.linear1.bias torch.Size([4096])\n",
      "layers.3.linear2.weight torch.Size([1024, 4096])\n",
      "layers.3.linear2.bias torch.Size([1024])\n",
      "layers.3.norm1.weight torch.Size([1024])\n",
      "layers.3.norm1.bias torch.Size([1024])\n",
      "layers.3.norm2.weight torch.Size([1024])\n",
      "layers.3.norm2.bias torch.Size([1024])\n",
      "layers.3.norm3.weight torch.Size([1024])\n",
      "layers.3.norm3.bias torch.Size([1024])\n",
      "layers.4.self_attn.in_proj_weight torch.Size([3072, 1024])\n",
      "layers.4.self_attn.in_proj_bias torch.Size([3072])\n",
      "layers.4.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "layers.4.self_attn.out_proj.bias torch.Size([1024])\n",
      "layers.4.multihead_attn.in_proj_weight torch.Size([3072, 1024])\n",
      "layers.4.multihead_attn.in_proj_bias torch.Size([3072])\n",
      "layers.4.multihead_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "layers.4.multihead_attn.out_proj.bias torch.Size([1024])\n",
      "layers.4.linear1.weight torch.Size([4096, 1024])\n",
      "layers.4.linear1.bias torch.Size([4096])\n",
      "layers.4.linear2.weight torch.Size([1024, 4096])\n",
      "layers.4.linear2.bias torch.Size([1024])\n",
      "layers.4.norm1.weight torch.Size([1024])\n",
      "layers.4.norm1.bias torch.Size([1024])\n",
      "layers.4.norm2.weight torch.Size([1024])\n",
      "layers.4.norm2.bias torch.Size([1024])\n",
      "layers.4.norm3.weight torch.Size([1024])\n",
      "layers.4.norm3.bias torch.Size([1024])\n",
      "layers.5.self_attn.in_proj_weight torch.Size([3072, 1024])\n",
      "layers.5.self_attn.in_proj_bias torch.Size([3072])\n",
      "layers.5.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "layers.5.self_attn.out_proj.bias torch.Size([1024])\n",
      "layers.5.multihead_attn.in_proj_weight torch.Size([3072, 1024])\n",
      "layers.5.multihead_attn.in_proj_bias torch.Size([3072])\n",
      "layers.5.multihead_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "layers.5.multihead_attn.out_proj.bias torch.Size([1024])\n",
      "layers.5.linear1.weight torch.Size([4096, 1024])\n",
      "layers.5.linear1.bias torch.Size([4096])\n",
      "layers.5.linear2.weight torch.Size([1024, 4096])\n",
      "layers.5.linear2.bias torch.Size([1024])\n",
      "layers.5.norm1.weight torch.Size([1024])\n",
      "layers.5.norm1.bias torch.Size([1024])\n",
      "layers.5.norm2.weight torch.Size([1024])\n",
      "layers.5.norm2.bias torch.Size([1024])\n",
      "layers.5.norm3.weight torch.Size([1024])\n",
      "layers.5.norm3.bias torch.Size([1024])\n",
      "norm.weight torch.Size([1024])\n",
      "norm.bias torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "for key, value in model.decoder.state_dict().items():\n",
    "    #for p in layer.modules():\n",
    "    print(key, value.shape)\n",
    "    #break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意：\n",
    "encoder里的`norm2` 实际上是decoder里的`norm3`\n",
    "\n",
    "encoder里的`dropout2` 实际上是decoder里的`dropout3`\n",
    "\n",
    "所以直接 用 `load_state_dict(dict, strict = False)` 是不行的\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'load_state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m decoder_state_dict \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mstate_dict()\n\u001b[0;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49mLM_decoder\u001b[39m.\u001b[39;49mload_state_dict(decoder_state_dict, strict\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'load_state_dict'"
     ]
    }
   ],
   "source": [
    "decoder_state_dict = model.decoder.state_dict()\n",
    "model.LM_decoder.load_state_dict(decoder_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['layers.0.multihead_attn.in_proj_weight', 'layers.0.multihead_attn.in_proj_bias', 'layers.0.multihead_attn.out_proj.weight', 'layers.0.multihead_attn.out_proj.bias', 'layers.0.norm3.weight', 'layers.0.norm3.bias', 'layers.1.multihead_attn.in_proj_weight', 'layers.1.multihead_attn.in_proj_bias', 'layers.1.multihead_attn.out_proj.weight', 'layers.1.multihead_attn.out_proj.bias', 'layers.1.norm3.weight', 'layers.1.norm3.bias', 'layers.2.multihead_attn.in_proj_weight', 'layers.2.multihead_attn.in_proj_bias', 'layers.2.multihead_attn.out_proj.weight', 'layers.2.multihead_attn.out_proj.bias', 'layers.2.norm3.weight', 'layers.2.norm3.bias', 'layers.3.multihead_attn.in_proj_weight', 'layers.3.multihead_attn.in_proj_bias', 'layers.3.multihead_attn.out_proj.weight', 'layers.3.multihead_attn.out_proj.bias', 'layers.3.norm3.weight', 'layers.3.norm3.bias', 'layers.4.multihead_attn.in_proj_weight', 'layers.4.multihead_attn.in_proj_bias', 'layers.4.multihead_attn.out_proj.weight', 'layers.4.multihead_attn.out_proj.bias', 'layers.4.norm3.weight', 'layers.4.norm3.bias', 'layers.5.multihead_attn.in_proj_weight', 'layers.5.multihead_attn.in_proj_bias', 'layers.5.multihead_attn.out_proj.weight', 'layers.5.multihead_attn.out_proj.bias', 'layers.5.norm3.weight', 'layers.5.norm3.bias'], unexpected_keys=[])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LM_decoder_state_dict = model.LM_decoder.state_dict()\n",
    "model.decoder.load_state_dict(LM_decoder_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq import options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = options.get_interactive_generation_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cuda\n",
      "Training mode\n",
      "Loading checkpoint from /net/papilio/storage2/yhaoyuan/transformer_I2S/saved_model/LM/SpokenCOCO_LibriSpeech/23-02-15_13:53:27_sentence/checkpoint_coco_1_cap_per_img_1_min_word_freq.pth.tar\n",
      "Adjusting learning rate of group 0 to 4.9411e-07.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from models import TransformerLM, TransformerConditionedLM, TransformerSentenceLM\n",
    "from datasets import *\n",
    "from utils_LM import *       #changed\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "# from torcheval.metrics.text import Perplexity\n",
    "# from torch.optim.lr_scheduler import LambdaLR\n",
    "import shutil\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import yaml\n",
    "config_path = \"../../config_LM.yml\"\n",
    "with open(config_path, 'r') as yml:\n",
    "    config = yaml.safe_load(yml)\n",
    "\n",
    "dir_name = config[\"i2u\"][\"dir_name\"]\n",
    "model_params = config[\"i2u\"][\"model_params\"]\n",
    "train_params = config[\"i2u\"][\"train_params\"]\n",
    "\n",
    "# Data parameters\n",
    "# data_folder = '/media/ssd/caption data'  # folder with data files saved by create_input_files.py\n",
    "data_folder = f'../../data/processed/{dir_name}/'  # folder with data files saved by create_input_files.py\n",
    "# data_name = 'coco_4_cap_per_img_5_min_word_freq'  # base name shared by data files\n",
    "#data_name = f'coco_{str(config[\"i2u\"][\"captions_per_image\"])}_cap_per_img_{str(config[\"i2u\"][\"min_word_freq\"])}_min_word_freq'  # base name shared by data files\n",
    "data_name = f'coco_{str(config[\"i2u\"][\"captions_per_image\"])}_cap_per_img_{str(config[\"i2u\"][\"min_word_freq\"])}_min_word_freq'  # base name shared by data files\n",
    "\n",
    "# Model parameters\n",
    "# emb_dim = 512  # dimension of word embeddings\n",
    "# attention_dim = 512  # dimension of attention linear layers\n",
    "# decoder_dim = 512  # dimension of decoder RNN\n",
    "# dropout = 0.5\n",
    "# import torch\n",
    "# print(torch.__version__)\n",
    "# print(torch.version.cuda)\n",
    "# print(torch.cuda.is_available()) #查看cuda是否可用)\n",
    "# print(torch.backends.cudnn.version())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
    "# device = torch.device(\"cpu\")\n",
    "print(\"Training on device {}\".format(device.type))\n",
    "cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
    "\n",
    "# Training parameters\n",
    "start_epoch = 0\n",
    "epochs = train_params[\"epoch\"]  # number of epochs to train for (if early stopping is not triggered)\n",
    "batch_size = train_params[\"batch_size\"]\n",
    "workers = train_params[\"num_workers\"]  # for data-loading; right now, only 1 works with h5py\n",
    "# encoder_lr = 1e-4  # learning rate for encoder if fine-tuning\n",
    "# decoder_lr = 4e-4  # learning rate for decoder\n",
    "grad_clip = train_params[\"grad_clip\"]  # clip gradients at an absolute value of\n",
    "best_bleu4 = 0.  # BLEU-4 score right now\n",
    "best_accuracy = 0.\n",
    "best_perplexity = 100000\n",
    "print_freq = train_params[\"print_freq\"]  # print training/validation stats every __ batches\n",
    "# fine_tune_encoder = model_params[\"fine_tune_image_encoder\"]  # fine-tune encoder?\n",
    "checkpoint = train_params[\"checkpoint_path\"]  # path to checkpoint, None if none\n",
    "#checkpoint = \"/net/papilio/storage2/yhaoyuan/LAbyLM/model/I2U/gtts_3_captions/BEST_checkpoint_coco_3_cap_per_img_1_min_word_freq_gpu.pth.tar\"\n",
    "use_scheduler = train_params[\"use_scheduler\"]\n",
    "\n",
    "kl_weight = train_params[\"kl_weight\"]\n",
    "\n",
    "import sys\n",
    "is_debug = True if sys.gettrace() else False\n",
    "if is_debug:\n",
    "    print(\"Debugging Mode\")\n",
    "    train_ID = \"debugging_sentence\"\n",
    "else:\n",
    "    print(\"Training mode\")\n",
    "    train_ID = str(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) )[2:].replace(\" \", \"_\") + \"_sentence\"\n",
    "# train_ID = \"debugging\"\n",
    "\n",
    "# def get_lr_schedule(optimizer, num_warmup_epochs: int = 10, d_model: int = 2048):\n",
    "#     def lr_lambda(current_epoch: int):\n",
    "#         \"\"\"\n",
    "#         Eq. (3) in [Transformer paper](https://arxiv.org/abs/1706.03762)\n",
    "#         \"\"\"\n",
    "#         return d_model**(-0.5) * min((current_epoch+1)**(-0.5), (current_epoch+1)*num_warmup_epochs**(-1.5))\n",
    "\n",
    "#     return LambdaLR(optimizer, lr_lambda, verbose=True)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Training and validation.\n",
    "\"\"\"\n",
    "\n",
    "global best_bleu4, best_accuracy, best_perplexity, checkpoint, start_epoch, data_name, word_map, device\n",
    "\n",
    "# Read word map\n",
    "word_map_file = os.path.join(data_folder, 'WORDMAP_' + data_name + '.json')\n",
    "with open(word_map_file, 'r') as j:\n",
    "    word_map = json.load(j)\n",
    "\n",
    "# Initialize / load checkpoint\n",
    "model_params['vocab_size'] = len(word_map)\n",
    "model = TransformerLM(**model_params)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), train_params[\"lr\"])\n",
    "optimizer = getattr(torch.optim, train_params[\"optimizer\"])(model.parameters(), lr=train_params[\"lr\"])\n",
    "\n",
    "# scheduel LR\n",
    "# if use_scheduler:\n",
    "#     scheduler = get_lr_schedule(optimizer, train_params[\"warmup_epoch\"], model_params[\"d_model\"])\n",
    "if checkpoint is not None:\n",
    "    model, optimizer, start_epoch, best_bleu4, best_accuracy, best_perplexity = load_checkpoint(checkpoint, model, optimizer, device)\n",
    "\n",
    "if use_scheduler:\n",
    "    scheduler = get_lr_schedule(optimizer, train_params[\"warmup_epoch\"], model_params[\"d_model\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.last_epoch = start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/papilio/storage2/yhaoyuan/anaconda3/envs/encodec/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.4938562148434215e-06]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler.get_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "encodec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "094c9ae320a962664ae725268b2caa008c72eeabdfac83f71f78104cf05452c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
