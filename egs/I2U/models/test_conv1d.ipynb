{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Any, Union, Callable\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(32, 150, 1024)\n",
    "conv1_layer = nn.Conv1d(in_channels=1024, out_channels=1024, kernel_size=1)\n",
    "linear_layer = nn.Linear(1024, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = conv1_layer(input[:,:64,:].permute(0,2,1)).permute(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64, 1024])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "input.shape\n",
    "output.shape\n",
    "assert input.shape == output.shape == input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 1024, 1])\n",
      "torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "for p in conv1_layer.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "for p in linear_layer.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_embedders = nn.ModuleList([nn.Conv1d(in_channels=1024, out_channels=1024, kernel_size=1) for i in range(12)])\n",
    "value_embedders = nn.ModuleList([nn.Conv1d(in_channels=1024, out_channels=1024, kernel_size=1) for i in range(12)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 1024, 1])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024, 1])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024, 1])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024, 1])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024, 1])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024, 1])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024, 1])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024, 1])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024, 1])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024, 1])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024, 1])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 1024, 1])\n",
      "torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "for p in key_embedders.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class prefix_TransformerEncoderLayer(nn.TransformerEncoderLayer):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_model: int, \n",
    "        nhead: int, \n",
    "        dim_feedforward: int = 2048, \n",
    "        dropout: float = 0.1, \n",
    "        activation: Union[str, Callable[[Tensor], Tensor]] = F.relu, \n",
    "        layer_norm_eps: float = 0.00001, \n",
    "        batch_first: bool = False, \n",
    "        norm_first: bool = False, \n",
    "        device=None, \n",
    "        dtype=None,\n",
    "        ) -> None:\n",
    "        super().__init__(\n",
    "            d_model, \n",
    "            nhead, \n",
    "            dim_feedforward, \n",
    "            dropout, \n",
    "            activation, \n",
    "            layer_norm_eps, \n",
    "            batch_first, \n",
    "            norm_first, \n",
    "            device, \n",
    "            dtype\n",
    "        )\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # 可能需要独立在 其之外： 因为需要frozen 整个LM的参数 还需要加载\n",
    "        # self.key_prompt_embed = nn.Linear(d_model, d_model)\n",
    "        # self.value_promt_embed = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            src: Tensor,\n",
    "            key_prompt: Tensor,\n",
    "            value_prompt: Tensor,\n",
    "            src_mask: Optional[Tensor] = None,\n",
    "            src_key_padding_mask: Optional[Tensor] = None\n",
    "        ) -> Tensor:\n",
    "        \"\"\"\n",
    "            Customized Encoder Layer.\n",
    "            key_prompt: Prefix-prompt key. It should have the same size as \"src\"\n",
    "            value_prompt: Prefix-prompt value. It should have the same size as \"src\"\n",
    "            For e.g.:\n",
    "                The original seq length is T; the prefix length is T'.\n",
    "                Concaten seq length should be T'+T, while the [B, :T', dim] is promptable, which\n",
    "                means, controled by params.\n",
    "                And in each layer of Transformer, the QKV is shaped like [B, T'+T, dim], so the \n",
    "                prefix part should also be promptable.\n",
    "\n",
    "            If key_prompt == value_prompt == x, this equals to a standard TransformerEncoderLayer.\n",
    "        \"\"\"\n",
    "        x = src\n",
    "        assert x.shape == key_prompt.shape == value_prompt.shape, f\"Q {x.shape}, K {key_prompt.shape}, V {value_prompt.shape} should have the same size\"\n",
    "        if self.norm_first:\n",
    "            # x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask)\n",
    "            x = x + self._sa_block_prompt(\n",
    "                x = self.norm1(x), \n",
    "                k_prompt = self.norm1(key_prompt), \n",
    "                v_prompt = self.norm1(value_prompt), \n",
    "                attn_mask = src_mask, \n",
    "                key_padding_mask = src_key_padding_mask\n",
    "            ) \n",
    "            x = x + self._ff_block(self.norm2(x))\n",
    "        else:\n",
    "            x = self.norm1(x + self._sa_block_prompt(x, key_prompt, value_prompt, src_mask, src_key_padding_mask))\n",
    "            x = self.norm2(x + self._ff_block(x))\n",
    "        return x\n",
    "\n",
    "    def _sa_block_prompt(\n",
    "            self,\n",
    "            x: Tensor,\n",
    "            k_prompt: Tensor,\n",
    "            v_prompt: Tensor,\n",
    "            attn_mask: Optional[Tensor], \n",
    "            key_padding_mask: Optional[Tensor]\n",
    "        ) -> Tensor:\n",
    "        \"\"\" \n",
    "        Customized Self-attention Block for prefix prompt tuning.\n",
    "        Ref: Prefix-tuning https://arxiv.org/abs/2101.00190\n",
    "        \"\"\"\n",
    "        x = self.self_attn(query=x, key=k_prompt, value=v_prompt,\n",
    "                           attn_mask=attn_mask,\n",
    "                           key_padding_mask=key_padding_mask,\n",
    "                           need_weights=False)[0]\n",
    "        return self.dropout1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = prefix_TransformerEncoderLayer(d_model=1024, nhead=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(3,3, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7913,  0.7482,  0.4389],\n",
       "        [ 2.0273,  0.2695, -0.3742],\n",
       "        [ 0.0391,  0.1879, -2.4662]], requires_grad=True)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_2 = input.clone()\n",
    "# input_2[0,0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7913,  0.7482,  0.4389],\n",
       "        [ 2.0273,  0.2695, -0.3742],\n",
       "        [ 0.0391,  0.1879, -2.4662]], requires_grad=True)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2559,  0.1615, -1.3387],\n",
       "        [ 2.0273,  0.2695, -0.3742],\n",
       "        [ 0.0391,  0.1879, -2.4662]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_simple = nn.Linear(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = linear_simple(input_2[:1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4677, -0.4245, -0.2293]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_2[:1,:] = prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4677, -0.4245, -0.2293],\n",
       "        [ 2.0273,  0.2695, -0.3742],\n",
       "        [ 0.0391,  0.1879, -2.4662]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "encodec-1.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
