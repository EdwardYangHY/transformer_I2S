{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "from models import TransformerLM, TransformerConditionedLM\n",
    "from models_modified import TransformerSentenceLM_FixedImg_gated\n",
    "import torch.nn.functional as F\n",
    "from fairseq import checkpoint_utils, options, tasks, utils\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = '../../config_sentence.yml'\n",
    "with open(config_path, 'r') as yml:\n",
    "    config = yaml.safe_load(yml)\n",
    "\n",
    "model_params = config[\"i2u\"][\"model_params\"]\n",
    "model_params['vocab_size'] = 1017\n",
    "img_refine_params = config[\"i2u\"][\"refine_encoder_params\"]\n",
    "model_params[\"refine_encoder_params\"] = img_refine_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerSentenceLM_FixedImg_gated(\n",
       "  (embed): Embedding(1017, 1024)\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (LM_decoder): None\n",
       "  (classifier): Linear(in_features=1024, out_features=1017, bias=True)\n",
       "  (image_encoder): DinoResEncoder_NoPool(\n",
       "    (resnet): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): custom_TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): custom_TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): custom_TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): custom_TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): custom_TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): custom_TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (6): custom_TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (7): custom_TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (8): custom_TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (9): custom_TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (10): custom_TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (11): custom_TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (image_encoder_embedding): Linear(in_features=2048, out_features=1024, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransformerSentenceLM_FixedImg_gated(**model_params)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LM_checkpoint = \"/net/papilio/storage2/yhaoyuan/transformer_I2S/saved_model/LM/SpokenCOCO_LibriSpeech/PP_15.6512/checkpoint_coco_1_cap_per_img_1_min_word_freq.pth.tar\"\n",
    "LM_state_dict = torch.load(LM_checkpoint)\n",
    "LM_state_dict = LM_state_dict[\"model_state_dict\"]\n",
    "\n",
    "# LM_checkpoint2 = \"/net/papilio/storage2/yhaoyuan/transformer_I2S/saved_model/LM/SpokenCOCO_LibriSpeech/PP_15.8054/checkpoint_coco_1_cap_per_img_1_min_word_freq.pth.tar\"\n",
    "# LM_state_dict2 = torch.load(LM_checkpoint2)\n",
    "# LM_state_dict2 = LM_state_dict2[\"model_state_dict\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprefix\\n    None                                        None\\n\\n    embed.weight                                embed.weight\\n    pos_encoder.pe                              pos_encoder.pe\\n\\nFor each Layer:\\nPrefix:\\n    decoder.layers.[]                           LM_decoder.layers.[]\\n\\n    norm1.weight                                norm1.weight\\n    norm1.bias                                  norm1.bias\\n\\n    self_attn.in_proj_weight                    self_attn.in_proj_weight\\n    self_attn.in_proj_bias                      self_attn.in_proj_bias\\n    self_attn.out_proj.weight                   self_attn.out_proj.weight\\n    self_attn.out_proj.bias                     self_attn.out_proj.bias\\n\\n    norm2.weight                                a\\n    norm2.bias                                  a\\n\\n    multihead_attn.in_proj_weight               a\\n    multihead_attn.in_proj_bias                 a\\n    multihead_attn.out_proj.weight              a\\n    multihead_attn.out_proj.bias                a\\n\\n    norm3.weight                                norm2.weight\\n    norm3.bias                                  norm2.bias\\n    \\n    linear1.weight                              linear1.weight \\n    linear1.bias                                linear1.bias\\n    linear2.weight                              linear2.weight \\n    linear2.bias                                linear2.bias \\n    \\n\\nFor Final Layer:\\nprefix:\\n    decoder.                                    LM_decoder\\n\\n    norm.weight                                 norm.weight\\n    norm.bias                                   norm.bias\\n\\nClassifier:\\n    classifier.weight                           classifier.weight\\n    classifier.bias                             classifier.bias\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "prefix\n",
    "    None                                        None\n",
    "\n",
    "    embed.weight                                embed.weight\n",
    "    pos_encoder.pe                              pos_encoder.pe\n",
    "\n",
    "For each Layer:\n",
    "Prefix:\n",
    "    decoder.layers.[]                           LM_decoder.layers.[]\n",
    "\n",
    "    norm1.weight                                norm1.weight\n",
    "    norm1.bias                                  norm1.bias\n",
    "\n",
    "    self_attn.in_proj_weight                    self_attn.in_proj_weight\n",
    "    self_attn.in_proj_bias                      self_attn.in_proj_bias\n",
    "    self_attn.out_proj.weight                   self_attn.out_proj.weight\n",
    "    self_attn.out_proj.bias                     self_attn.out_proj.bias\n",
    "\n",
    "    norm2.weight                                a\n",
    "    norm2.bias                                  a\n",
    "\n",
    "    multihead_attn.in_proj_weight               a\n",
    "    multihead_attn.in_proj_bias                 a\n",
    "    multihead_attn.out_proj.weight              a\n",
    "    multihead_attn.out_proj.bias                a\n",
    "\n",
    "    norm3.weight                                norm2.weight\n",
    "    norm3.bias                                  norm2.bias\n",
    "    \n",
    "    linear1.weight                              linear1.weight \n",
    "    linear1.bias                                linear1.bias\n",
    "    linear2.weight                              linear2.weight \n",
    "    linear2.bias                                linear2.bias \n",
    "    \n",
    "\n",
    "For Final Layer:\n",
    "prefix:\n",
    "    decoder.                                    LM_decoder\n",
    "\n",
    "    norm.weight                                 norm.weight\n",
    "    norm.bias                                   norm.bias\n",
    "\n",
    "Classifier:\n",
    "    classifier.weight                           classifier.weight\n",
    "    classifier.bias                             classifier.bias\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_key(tgt_state_dict, key, value):\n",
    "    model = tgt_state_dict\n",
    "    # Make sure the loaded values won't cause errors\n",
    "    assert model[key].shape == value.shape, f\"Key {key}, need shape {model[key].shape}, get shape {value.shape}\"\n",
    "    assert model[key].dtype == value.dtype, f\"Key {key}, need type {model[key].dtype}, get type {value.dtype}\"\n",
    "    model[key] = value\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_layer(tgt_state_dict, src_state_dict, layer_id):\n",
    "    # Load in_proj weight and bias\n",
    "    tgt_prefix = f\"decoder.layers.{int(layer_id)}.\"\n",
    "    src_prefix = f\"LM_decoder.layers.{int(layer_id)}.\"\n",
    "\n",
    "    tgt_state_dict = load_key(tgt_state_dict, tgt_prefix+\"norm1.weight\", src_state_dict[src_prefix + \"norm1.weight\"])\n",
    "    tgt_state_dict = load_key(tgt_state_dict, tgt_prefix+\"norm1.bias\", src_state_dict[src_prefix + \"norm1.bias\"])\n",
    "\n",
    "    tgt_state_dict = load_key(tgt_state_dict, tgt_prefix+\"self_attn.in_proj_weight\", src_state_dict[src_prefix + \"self_attn.in_proj_weight\"])\n",
    "    tgt_state_dict = load_key(tgt_state_dict, tgt_prefix+\"self_attn.in_proj_bias\", src_state_dict[src_prefix + \"self_attn.in_proj_bias\"])\n",
    "    tgt_state_dict = load_key(tgt_state_dict, tgt_prefix+\"self_attn.out_proj.weight\", src_state_dict[src_prefix + \"self_attn.out_proj.weight\"])\n",
    "    tgt_state_dict = load_key(tgt_state_dict, tgt_prefix+\"self_attn.out_proj.bias\", src_state_dict[src_prefix + \"self_attn.out_proj.bias\"])\n",
    "\n",
    "    tgt_state_dict = load_key(tgt_state_dict, tgt_prefix+\"norm3.weight\", src_state_dict[src_prefix + \"norm2.weight\"])\n",
    "    tgt_state_dict = load_key(tgt_state_dict, tgt_prefix+\"norm3.bias\", src_state_dict[src_prefix + \"norm2.bias\"])\n",
    "    \n",
    "    tgt_state_dict = load_key(tgt_state_dict, tgt_prefix+\"linear1.weight\", src_state_dict[src_prefix + \"linear1.weight\"])\n",
    "    tgt_state_dict = load_key(tgt_state_dict, tgt_prefix+\"linear1.bias\", src_state_dict[src_prefix + \"linear1.bias\"])\n",
    "    tgt_state_dict = load_key(tgt_state_dict, tgt_prefix+\"linear2.weight\", src_state_dict[src_prefix + \"linear2.weight\"])\n",
    "    tgt_state_dict = load_key(tgt_state_dict, tgt_prefix+\"linear2.bias\", src_state_dict[src_prefix + \"linear2.bias\"])\n",
    "    return tgt_state_dict\n",
    "\n",
    "def load_embed(tgt_state_dict, src_state_dict):\n",
    "    tgt_state_dict = load_key(tgt_state_dict, \"embed.weight\", src_state_dict[\"embed.weight\"])\n",
    "    tgt_state_dict = load_key(tgt_state_dict, \"classifier.weight\", src_state_dict[\"classifier.weight\"])\n",
    "    tgt_state_dict = load_key(tgt_state_dict, \"classifier.bias\", src_state_dict[\"classifier.bias\"])\n",
    "    return tgt_state_dict\n",
    "\n",
    "def load_final_norm(tgt_state_dict, src_state_dict):\n",
    "    tgt_state_dict = load_key(tgt_state_dict, \"decoder.norm.weight\", src_state_dict[\"LM_decoder.norm.weight\"])\n",
    "    tgt_state_dict = load_key(tgt_state_dict, \"decoder.norm.bias\", src_state_dict[\"LM_decoder.norm.bias\"])\n",
    "    return tgt_state_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict = load_embed(model_state_dict, LM_state_dict)\n",
    "model_state_dict = load_final_norm(model_state_dict, LM_state_dict)\n",
    "for i in range(12):\n",
    "    model_state_dict = load_layer(model_state_dict, LM_state_dict, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 10\n",
    "check_dict = {\n",
    "    \"embed.weight\": \"embed.weight\",\n",
    "\n",
    "    f\"decoder.layers.{a}.norm1.weight\": f\"LM_decoder.layers.{a}.norm1.weight\",\n",
    "    f\"decoder.layers.{a}.norm1.bias\": f\"LM_decoder.layers.{a}.norm1.bias\",\n",
    "    f\"decoder.layers.{a}.self_attn.in_proj_weight\": f\"LM_decoder.layers.{a}.self_attn.in_proj_weight\",\n",
    "    f\"decoder.layers.{a}.self_attn.in_proj_bias\": f\"LM_decoder.layers.{a}.self_attn.in_proj_bias\",\n",
    "    f\"decoder.layers.{a}.self_attn.out_proj.weight\": f\"LM_decoder.layers.{a}.self_attn.out_proj.weight\",\n",
    "    f\"decoder.layers.{a}.self_attn.out_proj.bias\": f\"LM_decoder.layers.{a}.self_attn.out_proj.bias\",\n",
    "    f\"decoder.layers.{a}.norm3.weight\": f\"LM_decoder.layers.{a}.norm2.weight\",\n",
    "    f\"decoder.layers.{a}.norm3.bias\": f\"LM_decoder.layers.{a}.norm2.bias\",\n",
    "    f\"decoder.layers.{a}.linear1.weight\": f\"LM_decoder.layers.{a}.linear1.weight\",\n",
    "    f\"decoder.layers.{a}.linear1.bias\": f\"LM_decoder.layers.{a}.linear1.bias\",\n",
    "    f\"decoder.layers.{a}.linear2.weight\": f\"LM_decoder.layers.{a}.linear2.weight\",\n",
    "    f\"decoder.layers.{a}.linear2.bias\": f\"LM_decoder.layers.{a}.linear2.bias\",\n",
    "\n",
    "    \"decoder.norm.weight\": \"LM_decoder.norm.weight\",\n",
    "    \"decoder.norm.bias\": \"LM_decoder.norm.bias\",\n",
    "    \"classifier.weight\": \"classifier.weight\",\n",
    "    \"classifier.bias\": \"classifier.bias\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.state_dict()\n",
    "for k,v in check_dict.items():\n",
    "    check = model.state_dict()[k] == LM_state_dict[v]\n",
    "    print(check.all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pos encoder 没有加载 但是是一样的\n",
    "# 说明pos确实不需要加载。\n",
    "(model.state_dict()[\"pos_encoder.pe\"] == LM_state_dict[\"pos_encoder.pe\"]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed.weight\n",
      "pos_encoder.pe\n",
      "classifier.weight\n",
      "classifier.bias\n",
      "decoder.layers.0.self_attn.in_proj_weight\n",
      "decoder.layers.0.self_attn.in_proj_bias\n",
      "decoder.layers.0.self_attn.out_proj.weight\n",
      "decoder.layers.0.self_attn.out_proj.bias\n",
      "decoder.layers.0.multihead_attn.in_proj_weight\n",
      "decoder.layers.0.multihead_attn.in_proj_bias\n",
      "decoder.layers.0.multihead_attn.out_proj.weight\n",
      "decoder.layers.0.multihead_attn.out_proj.bias\n",
      "decoder.layers.0.linear1.weight\n",
      "decoder.layers.0.linear1.bias\n",
      "decoder.layers.0.linear2.weight\n",
      "decoder.layers.0.linear2.bias\n",
      "decoder.layers.0.norm1.weight\n",
      "decoder.layers.0.norm1.bias\n",
      "decoder.layers.0.norm2.weight\n",
      "decoder.layers.0.norm2.bias\n",
      "decoder.layers.0.norm3.weight\n",
      "decoder.layers.0.norm3.bias\n",
      "decoder.layers.1.self_attn.in_proj_weight\n",
      "decoder.layers.1.self_attn.in_proj_bias\n",
      "decoder.layers.1.self_attn.out_proj.weight\n",
      "decoder.layers.1.self_attn.out_proj.bias\n",
      "decoder.layers.1.multihead_attn.in_proj_weight\n",
      "decoder.layers.1.multihead_attn.in_proj_bias\n",
      "decoder.layers.1.multihead_attn.out_proj.weight\n",
      "decoder.layers.1.multihead_attn.out_proj.bias\n",
      "decoder.layers.1.linear1.weight\n",
      "decoder.layers.1.linear1.bias\n",
      "decoder.layers.1.linear2.weight\n",
      "decoder.layers.1.linear2.bias\n",
      "decoder.layers.1.norm1.weight\n",
      "decoder.layers.1.norm1.bias\n",
      "decoder.layers.1.norm2.weight\n",
      "decoder.layers.1.norm2.bias\n",
      "decoder.layers.1.norm3.weight\n",
      "decoder.layers.1.norm3.bias\n",
      "decoder.layers.2.self_attn.in_proj_weight\n",
      "decoder.layers.2.self_attn.in_proj_bias\n",
      "decoder.layers.2.self_attn.out_proj.weight\n",
      "decoder.layers.2.self_attn.out_proj.bias\n",
      "decoder.layers.2.multihead_attn.in_proj_weight\n",
      "decoder.layers.2.multihead_attn.in_proj_bias\n",
      "decoder.layers.2.multihead_attn.out_proj.weight\n",
      "decoder.layers.2.multihead_attn.out_proj.bias\n",
      "decoder.layers.2.linear1.weight\n",
      "decoder.layers.2.linear1.bias\n",
      "decoder.layers.2.linear2.weight\n",
      "decoder.layers.2.linear2.bias\n",
      "decoder.layers.2.norm1.weight\n",
      "decoder.layers.2.norm1.bias\n",
      "decoder.layers.2.norm2.weight\n",
      "decoder.layers.2.norm2.bias\n",
      "decoder.layers.2.norm3.weight\n",
      "decoder.layers.2.norm3.bias\n",
      "decoder.layers.3.self_attn.in_proj_weight\n",
      "decoder.layers.3.self_attn.in_proj_bias\n",
      "decoder.layers.3.self_attn.out_proj.weight\n",
      "decoder.layers.3.self_attn.out_proj.bias\n",
      "decoder.layers.3.multihead_attn.in_proj_weight\n",
      "decoder.layers.3.multihead_attn.in_proj_bias\n",
      "decoder.layers.3.multihead_attn.out_proj.weight\n",
      "decoder.layers.3.multihead_attn.out_proj.bias\n",
      "decoder.layers.3.linear1.weight\n",
      "decoder.layers.3.linear1.bias\n",
      "decoder.layers.3.linear2.weight\n",
      "decoder.layers.3.linear2.bias\n",
      "decoder.layers.3.norm1.weight\n",
      "decoder.layers.3.norm1.bias\n",
      "decoder.layers.3.norm2.weight\n",
      "decoder.layers.3.norm2.bias\n",
      "decoder.layers.3.norm3.weight\n",
      "decoder.layers.3.norm3.bias\n",
      "decoder.layers.4.self_attn.in_proj_weight\n",
      "decoder.layers.4.self_attn.in_proj_bias\n",
      "decoder.layers.4.self_attn.out_proj.weight\n",
      "decoder.layers.4.self_attn.out_proj.bias\n",
      "decoder.layers.4.multihead_attn.in_proj_weight\n",
      "decoder.layers.4.multihead_attn.in_proj_bias\n",
      "decoder.layers.4.multihead_attn.out_proj.weight\n",
      "decoder.layers.4.multihead_attn.out_proj.bias\n",
      "decoder.layers.4.linear1.weight\n",
      "decoder.layers.4.linear1.bias\n",
      "decoder.layers.4.linear2.weight\n",
      "decoder.layers.4.linear2.bias\n",
      "decoder.layers.4.norm1.weight\n",
      "decoder.layers.4.norm1.bias\n",
      "decoder.layers.4.norm2.weight\n",
      "decoder.layers.4.norm2.bias\n",
      "decoder.layers.4.norm3.weight\n",
      "decoder.layers.4.norm3.bias\n",
      "decoder.layers.5.self_attn.in_proj_weight\n",
      "decoder.layers.5.self_attn.in_proj_bias\n",
      "decoder.layers.5.self_attn.out_proj.weight\n",
      "decoder.layers.5.self_attn.out_proj.bias\n",
      "decoder.layers.5.multihead_attn.in_proj_weight\n",
      "decoder.layers.5.multihead_attn.in_proj_bias\n",
      "decoder.layers.5.multihead_attn.out_proj.weight\n",
      "decoder.layers.5.multihead_attn.out_proj.bias\n",
      "decoder.layers.5.linear1.weight\n",
      "decoder.layers.5.linear1.bias\n",
      "decoder.layers.5.linear2.weight\n",
      "decoder.layers.5.linear2.bias\n",
      "decoder.layers.5.norm1.weight\n",
      "decoder.layers.5.norm1.bias\n",
      "decoder.layers.5.norm2.weight\n",
      "decoder.layers.5.norm2.bias\n",
      "decoder.layers.5.norm3.weight\n",
      "decoder.layers.5.norm3.bias\n",
      "decoder.layers.6.self_attn.in_proj_weight\n",
      "decoder.layers.6.self_attn.in_proj_bias\n",
      "decoder.layers.6.self_attn.out_proj.weight\n",
      "decoder.layers.6.self_attn.out_proj.bias\n",
      "decoder.layers.6.multihead_attn.in_proj_weight\n",
      "decoder.layers.6.multihead_attn.in_proj_bias\n",
      "decoder.layers.6.multihead_attn.out_proj.weight\n",
      "decoder.layers.6.multihead_attn.out_proj.bias\n",
      "decoder.layers.6.linear1.weight\n",
      "decoder.layers.6.linear1.bias\n",
      "decoder.layers.6.linear2.weight\n",
      "decoder.layers.6.linear2.bias\n",
      "decoder.layers.6.norm1.weight\n",
      "decoder.layers.6.norm1.bias\n",
      "decoder.layers.6.norm2.weight\n",
      "decoder.layers.6.norm2.bias\n",
      "decoder.layers.6.norm3.weight\n",
      "decoder.layers.6.norm3.bias\n",
      "decoder.layers.7.self_attn.in_proj_weight\n",
      "decoder.layers.7.self_attn.in_proj_bias\n",
      "decoder.layers.7.self_attn.out_proj.weight\n",
      "decoder.layers.7.self_attn.out_proj.bias\n",
      "decoder.layers.7.multihead_attn.in_proj_weight\n",
      "decoder.layers.7.multihead_attn.in_proj_bias\n",
      "decoder.layers.7.multihead_attn.out_proj.weight\n",
      "decoder.layers.7.multihead_attn.out_proj.bias\n",
      "decoder.layers.7.linear1.weight\n",
      "decoder.layers.7.linear1.bias\n",
      "decoder.layers.7.linear2.weight\n",
      "decoder.layers.7.linear2.bias\n",
      "decoder.layers.7.norm1.weight\n",
      "decoder.layers.7.norm1.bias\n",
      "decoder.layers.7.norm2.weight\n",
      "decoder.layers.7.norm2.bias\n",
      "decoder.layers.7.norm3.weight\n",
      "decoder.layers.7.norm3.bias\n",
      "decoder.layers.8.self_attn.in_proj_weight\n",
      "decoder.layers.8.self_attn.in_proj_bias\n",
      "decoder.layers.8.self_attn.out_proj.weight\n",
      "decoder.layers.8.self_attn.out_proj.bias\n",
      "decoder.layers.8.multihead_attn.in_proj_weight\n",
      "decoder.layers.8.multihead_attn.in_proj_bias\n",
      "decoder.layers.8.multihead_attn.out_proj.weight\n",
      "decoder.layers.8.multihead_attn.out_proj.bias\n",
      "decoder.layers.8.linear1.weight\n",
      "decoder.layers.8.linear1.bias\n",
      "decoder.layers.8.linear2.weight\n",
      "decoder.layers.8.linear2.bias\n",
      "decoder.layers.8.norm1.weight\n",
      "decoder.layers.8.norm1.bias\n",
      "decoder.layers.8.norm2.weight\n",
      "decoder.layers.8.norm2.bias\n",
      "decoder.layers.8.norm3.weight\n",
      "decoder.layers.8.norm3.bias\n",
      "decoder.layers.9.self_attn.in_proj_weight\n",
      "decoder.layers.9.self_attn.in_proj_bias\n",
      "decoder.layers.9.self_attn.out_proj.weight\n",
      "decoder.layers.9.self_attn.out_proj.bias\n",
      "decoder.layers.9.multihead_attn.in_proj_weight\n",
      "decoder.layers.9.multihead_attn.in_proj_bias\n",
      "decoder.layers.9.multihead_attn.out_proj.weight\n",
      "decoder.layers.9.multihead_attn.out_proj.bias\n",
      "decoder.layers.9.linear1.weight\n",
      "decoder.layers.9.linear1.bias\n",
      "decoder.layers.9.linear2.weight\n",
      "decoder.layers.9.linear2.bias\n",
      "decoder.layers.9.norm1.weight\n",
      "decoder.layers.9.norm1.bias\n",
      "decoder.layers.9.norm2.weight\n",
      "decoder.layers.9.norm2.bias\n",
      "decoder.layers.9.norm3.weight\n",
      "decoder.layers.9.norm3.bias\n",
      "decoder.layers.10.self_attn.in_proj_weight\n",
      "decoder.layers.10.self_attn.in_proj_bias\n",
      "decoder.layers.10.self_attn.out_proj.weight\n",
      "decoder.layers.10.self_attn.out_proj.bias\n",
      "decoder.layers.10.multihead_attn.in_proj_weight\n",
      "decoder.layers.10.multihead_attn.in_proj_bias\n",
      "decoder.layers.10.multihead_attn.out_proj.weight\n",
      "decoder.layers.10.multihead_attn.out_proj.bias\n",
      "decoder.layers.10.linear1.weight\n",
      "decoder.layers.10.linear1.bias\n",
      "decoder.layers.10.linear2.weight\n",
      "decoder.layers.10.linear2.bias\n",
      "decoder.layers.10.norm1.weight\n",
      "decoder.layers.10.norm1.bias\n",
      "decoder.layers.10.norm2.weight\n",
      "decoder.layers.10.norm2.bias\n",
      "decoder.layers.10.norm3.weight\n",
      "decoder.layers.10.norm3.bias\n",
      "decoder.layers.11.self_attn.in_proj_weight\n",
      "decoder.layers.11.self_attn.in_proj_bias\n",
      "decoder.layers.11.self_attn.out_proj.weight\n",
      "decoder.layers.11.self_attn.out_proj.bias\n",
      "decoder.layers.11.multihead_attn.in_proj_weight\n",
      "decoder.layers.11.multihead_attn.in_proj_bias\n",
      "decoder.layers.11.multihead_attn.out_proj.weight\n",
      "decoder.layers.11.multihead_attn.out_proj.bias\n",
      "decoder.layers.11.linear1.weight\n",
      "decoder.layers.11.linear1.bias\n",
      "decoder.layers.11.linear2.weight\n",
      "decoder.layers.11.linear2.bias\n",
      "decoder.layers.11.norm1.weight\n",
      "decoder.layers.11.norm1.bias\n",
      "decoder.layers.11.norm2.weight\n",
      "decoder.layers.11.norm2.bias\n",
      "decoder.layers.11.norm3.weight\n",
      "decoder.layers.11.norm3.bias\n",
      "decoder.norm.weight\n",
      "decoder.norm.bias\n"
     ]
    }
   ],
   "source": [
    "for k in model.state_dict().keys():\n",
    "    if k[:len(\"image_encoder\")] != \"image_encoder\":\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerSentenceLM_FixedImg_gated(\n",
       "  (embed): Embedding(1017, 1024)\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (LM_decoder): None\n",
       "  (classifier): Linear(in_features=1024, out_features=1017, bias=True)\n",
       "  (image_encoder): DinoResEncoder_NoPool(\n",
       "    (resnet): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): custom_TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): custom_TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): custom_TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): custom_TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): custom_TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): custom_TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (6): custom_TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (7): custom_TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (8): custom_TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (9): custom_TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (10): custom_TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (11): custom_TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (image_encoder_embedding): Linear(in_features=2048, out_features=1024, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = TransformerSentenceLM_FixedImg_gated(**model_params)\n",
    "model2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load uLM weights from path: /net/papilio/storage2/yhaoyuan/transformer_I2S/saved_model/LM/SpokenCOCO_LibriSpeech/PP_15.6512/checkpoint_coco_1_cap_per_img_1_min_word_freq.pth.tar\n"
     ]
    }
   ],
   "source": [
    "model2.load_Pretrained_LM(LM_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.state_dict()\n",
    "for k,v in check_dict.items():\n",
    "    check = model.state_dict()[k] == model2.state_dict()[k]\n",
    "    print(check.all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "encodec-1.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3533e64601b254dacaa9c268c9ab5d42dc0994555240d8afdbbd46ece987524"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
