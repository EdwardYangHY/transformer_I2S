data:
  # store with absolute path
  # not implemented yet
  # Real Voice leaveout number
  train_json: '/net/papilio/storage2/yhaoyuan/LAbyLM/new_dataset/train_trimmed_mapping.json'
  valid_json: '/net/papilio/storage2/yhaoyuan/LAbyLM/new_dataset/valid_trimmed_mapping.json'
  test_json: '/net/papilio/storage2/yhaoyuan/LAbyLM/new_dataset/test_trimmed_mapping.json'
  max_len: 100 # 150 # 200 for SpokenCOCO
  # image_resolution: 256

i2u:
  #### Used when inferencing
  model: "../../model/I2U/trimmed_mapping_SpeakerALL_Color/BEST_checkpoint_coco_2_cap_per_img_1_min_word_freq_gpu.pth.tar" # leave out colors
  wordmap: "../../data/processed/trimmed_mapping_SpeakerALL_Color/WORDMAP_coco_2_cap_per_img_1_min_word_freq.json" # leave out colors
  
  #### Used when training models
  captions_per_image: 4 #取决于 每一张图片对应多少个caption！比如 “Apple” "An apple in a white background" "It's an apple in a white background." 那就是3个caption
  min_word_freq: 1
  
  # dir_name: origin_4_captions_256_hubert_sentence
  # dir_name: origin_3_captions_256_hubert_sentence
  # dir_name: SpokenCOCO_5_captions_hubert_256
  # dir_name: komatsu_4_captions_256_hubert
  dir_name: komatsu_4_captions_224_hubert
  # dir_name: komatsu_4_captions_224_hubert_5_percent
  # dir_name: komatsu_4_captions_224_hubert_10_percent
  # dir_name: komatsu_4_captions_224_hubert_20_percent
  # dir_name: komatsu_4_captions_256_hubert_10_percent
  # dir_name: komatsu_4_captions_256_hubert_20_percent
  # dir_name: komatsu_4_captions_224_ResDAVEnet
  # dir_name: komatsu_4_captions_224_ResDAVEnet_10_percent
  # dir_name: komatsu_4_captions_224_ResDAVEnet_20_percent
  # dir_name: record_3_captions_224_hubert_there_sentence_10_speakers

  # captions_per_image: 2 
  # min_word_freq: 1
  # dir_name: trimmed_mapping_SpeakerALL
  
  # captions_per_image: 4 
  # min_word_freq: 1
  # dir_name: Komatsu_4_captions_all_224
  


  #### Model params
  # batch_size: 32
  # num_workers: 10
  # epoch: 80

  train_params:
    batch_size: 96 # 64
    num_workers: 10
    grad_clip: 5.
    lr: 1.0e-3  # 1.0e-3, set to 4.0e-4 if scheduler = False
    optimizer: "Adam" # [Adam, AdamW]
    epoch: 100 # 50
    warmup_epoch: 5 # 5 only make sense when use_scheduler = True
    use_scheduler: False # True
    print_freq: 100
    kl_weight: 0.01
    checkpoint_path: # /net/papilio/storage2/yhaoyuan/transformer_I2S/saved_model/I2U/VC_5_captions_224/fixed_img_1024_no_sentence/bleu-4_BEST_checkpoint_coco_5_cap_per_img_1_min_word_freq_gpu.pth.tar
    gated_decoder: False
    load_uLM: True
    freeze_uLM: True
    freeze_uLM_hard: False
    
    # ### SpokenCOCO + Libri-light small, ResDAVEnet-VQ, vocab: 1017
    # LM_checkpoint: ../../saved_model/LM/SpokenCOCO_LibriSpeech/PP_15.6512/checkpoint_coco_1_cap_per_img_1_min_word_freq.pth.tar
    
    # ### Libri-light Medium, 12 layers, HuBERT, vocab: 102
    # LM_checkpoint: ../../saved_model/LM/Libri_Light_small_hubert_256/perplexity_6/perplexity_BEST_checkpoint_coco_1_cap_per_img_1_min_word_freq_gpu.pth.tar
    
    ### Libri-light Medium, 6 layers, HuBERT, vocab: 102
    LM_checkpoint: ../../saved_model/LM/Libri_Light_medium_hubert_256/6_layers_ICASSP_perplexity_5.240/perplexity_BEST_checkpoint_coco_1_cap_per_img_1_min_word_freq_gpu.pth.tar

  model_params:
    d_model: 768 # 2048
    nhead: 8 # 16
    num_layers: 6 # 12
    activation: "gelu"
    layer_norm_eps: 1.0e-5
    batch_first: True
    norm_first : True
    dropout: 0.1
    image_backbone: "ViT" # "ResNet" or "ViT" or "ViT_all"
    use_sentence_encoder: True
    sentence_embed: 8 #16
    fine_tune_image_encoder: False # Fine tune ResNet's last few layers if image_backbone is "ResNet"
    use_refine_encoder: False
    use_global_feature: False
    AR: True

    sentence_encoder_num_layers: 6
    sentence_encoder_num_heads: 8
    global_mean_pooling: False # If false, just use the first token

  # refine_encoder_params:
  #   input_resolution: 14
  #   depth: 3
  #   num_heads: 8
  #   window_size: 14  # =14 退化为普通MSA结构
  #   shift_size: 0    # =0  无SW-MSA，仅W-MSA
  #   mlp_ratio: 4
  refine_encoder_params:
    input_resolution: 7 # 14
    depth: 3
    num_heads: 8
    window_size: 7  # =14 退化为普通MSA结构
    shift_size: 0    # =0  无SW-MSA，仅W-MSA
    mlp_ratio: 4

# 存储模型请将config一起存储
