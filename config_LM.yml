data:
  # store with absolute path
  # not implemented yet
  # Real Voice leaveout number
  train_json: '/net/papilio/storage2/yhaoyuan/LAbyLM/new_dataset/train_trimmed_mapping.json'
  valid_json: '/net/papilio/storage2/yhaoyuan/LAbyLM/new_dataset/valid_trimmed_mapping.json'
  test_json: '/net/papilio/storage2/yhaoyuan/LAbyLM/new_dataset/test_trimmed_mapping.json'
  max_len: 150
  image_resolution: 256

i2u:
  #### Used when inferencing
  model: "../../model/I2U/trimmed_mapping_SpeakerALL_Color/BEST_checkpoint_coco_2_cap_per_img_1_min_word_freq_gpu.pth.tar" # leave out colors
  wordmap: "../../data/processed/trimmed_mapping_SpeakerALL_Color/WORDMAP_coco_2_cap_per_img_1_min_word_freq.json" # leave out colors
  
  #### Used when training models
  captions_per_image: 1 
  min_word_freq: 1
  # dir_name: SpokenCOCO_LibriSpeech
  # dir_name: SpokenCOCO
  # dir_name: LibriSpeech
  # dir_name: VC_5_captions
  dir_name: Libri_Light_small_hubert_256

  #### Model params
  # batch_size: 32
  # num_workers: 10
  # epoch: 80

  train_params:
    batch_size: 20 # 128
    num_workers: 10
    grad_clip: 5.
    lr: 5.0e-4 # set to 4.0e-4 if scheduler = False
    optimizer: "Adam" # [Adam, AdamW]
    epoch: 100
    warmup_epoch: 10 # only make sense when use_scheduler = True
    use_scheduler: True
    print_freq: 1000
    kl_weight: 0.01
    checkpoint_path: #/net/papilio/storage2/yhaoyuan/transformer_I2S/saved_model/LM/Libri_Light_small_hubert_256/perplexity_6/perplexity_BEST_checkpoint_coco_1_cap_per_img_1_min_word_freq_gpu.pth.tar
  
  model_params:
    d_model: 2048 # 1028
    nhead: 16 # 32
    num_layers: 24 # 12
    activation: "gelu" # “relu”
    layer_norm_eps: 1.0e-5
    batch_first: True
    norm_first : True

  # refine_encoder_params:
  #   input_resolution: 14
  #   depth: 3
  #   num_heads: 8
  #   window_size: 14  # =14 退化为普通MSA结构
  #   shift_size: 0    # =0  无SW-MSA，仅W-MSA
  #   mlp_ratio: 4
  refine_encoder_params:
    input_resolution: 7 # 14
    depth: 3
    num_heads: 8
    window_size: 7  # =14 退化为普通MSA结构
    shift_size: 0    # =0  无SW-MSA，仅W-MSA
    mlp_ratio: 4

# 存储模型请将config一起存储
